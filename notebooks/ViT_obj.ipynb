{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Vision Transformer for Object Detection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Positional Encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementing sinusoidal positional encoding described in the Attention is all you need paper\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        # [batch_size, seq_len, 1]\n",
    "        pos_arr = torch.arange(start=0, step=1, end=seq_len).unsqueeze(0).repeat([batch_size, 1]).unsqueeze(2)\n",
    "        # [batch_size, 1, embed_dim]\n",
    "        i_arr = torch.arange(start=0, step=1, end=embed_dim).unsqueeze(0).repeat([batch_size, 1]).unsqueeze(1)\n",
    "        encoding = pos_arr / torch.pow(10000, 2 * i_arr / embed_dim)\n",
    "        # apply sinus to even idxs and cosine to odd\n",
    "        encoding[:, 0::2, :] = torch.sin(encoding[:, 0::2, :])\n",
    "        encoding[:, 1::2, :] = torch.cos(encoding[:, 1::2, :])\n",
    "        # todo - hacky and slow: do all processing on GPU / compute positional encoding once and save!\n",
    "        # return x + encoding\n",
    "        return x + encoding.cuda()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size, img_height, embed_dim = 1, 100, 200\n",
    "\n",
    "positional_encoding = SinusoidalPositionalEncoding()\n",
    "\n",
    "x = torch.zeros(size=[batch_size, img_height, embed_dim])\n",
    "encoding = positional_encoding(x).squeeze()\n",
    "plt.imshow(encoding, cmap=\"PiYG\")\n",
    "plt.ylabel(\"Position in sequence\"); plt.xlabel(\"embedding dim\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Todo - learnable position embedding wie im image is worth 16x16 words paper - einfach lernbarer parameter für positional embedding\n",
    "Hier könnte das sinusoidal embedding schon Sinn ergeben, da es ja tatsächlich eine Reihenfolge darstellt und nicht wie im Paper ein Grid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Patch Embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Receives input image (N, C, W, H) (assuming W=H) and turns it into a set of PxP patches\n",
    "    Each patch is mapped linearly to obtain a vector of the flattened patch\n",
    "\n",
    "    Implements equation (1) in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: list, num_patches: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = int(input_dim[2] / (num_patches ** 0.5))\n",
    "        self.embed_dim = embedding_dim\n",
    "        # by choosing kernel size and stride to be same as the patch size we divide the image implicitly into the\n",
    "        # desired patches\n",
    "        self.linear_projection = nn.Conv2d(input_dim[1], embedding_dim, kernel_size=self.patch_size,\n",
    "                                           stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # apply projection convolution and concatenate patches -> total of num_patches\n",
    "        x = self.linear_projection(x)\n",
    "        x = torch.reshape(x, [self.input_dim[0], self.num_patches, self.embed_dim])\n",
    "        return x\n",
    "\n",
    "class RowEmbedding(nn.Module):\n",
    "    # Todo - perhaps dont use single rows but 5 pixel wide rows\n",
    "\n",
    "    def __init__(self, input_dim: list, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # linear is always applied to last dimension (BxCxWxH)\n",
    "        # for now single projection is shared for each row!\n",
    "        self.proj = nn.Linear(input_dim[-1], embedding_dim, bias=True)\n",
    "        # Todo - missing positional encdoing\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        return torch.squeeze(self.proj(t))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dim = [16, 3, 256, 256]\n",
    "num_patches = 16\n",
    "embedding_dim = 768\n",
    "t = torch.randn(dim)\n",
    "patch_embedding = PatchEmbedding(dim, num_patches, embedding_dim)\n",
    "print(f\"Number of parameters: {count_parameters(patch_embedding):,}\")\n",
    "out = patch_embedding(t)\n",
    "print(\"Output shape:\", out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dim = [16, 1, 128, 256]\n",
    "embedding_dim = 168\n",
    "t = torch.randn(dim)\n",
    "row_embedding = RowEmbedding(dim, embedding_dim)\n",
    "out = row_embedding(t)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(f\"Number of parameters: {count_parameters(row_embedding):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Masked Self-Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Head Self-Attention module as described in the Attention is all you need paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, num_heads: int) -> None:\n",
    "        \"\"\"\n",
    "        :param embedding_dim: dimension of the word embeddings, must be dividable by num_heads\n",
    "        :param num_heads: number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0, \"Embedding dim must be dividable by number of attention heads!\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.embedding_dim // self.num_heads\n",
    "\n",
    "        # mapping matrices W_Q, W_K, W_V for queries, keys, values\n",
    "        # doing this in a single linear mapping for qkv is more efficient, but this notebook is mainly for readability\n",
    "        self.query_mapping = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.key_mapping = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.value_mapping = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        # scaling factor for more stable gradients when using the softmax function -> refer to paper for details\n",
    "        self.scaling_factor = math.sqrt(embedding_dim)\n",
    "\n",
    "        self.multi_head_mapping = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2:torch.Tensor, x3: torch.Tensor, mask: torch.Tensor= None, masked_val: float=1e15) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x1.shape\n",
    "        q = self.query_mapping(x1)\n",
    "        k = self.key_mapping(x2)\n",
    "        v = self.value_mapping(x3)\n",
    "        # reshape to heads format and [batch_size, seq_len, num_heads, head_dim] -> [batch_size, seq_len, num_heads, head_dim]\n",
    "        q = torch.reshape(q, shape=(batch_size, seq_len, self.num_heads, self.head_dim)).permute([0, 2, 1, 3])\n",
    "        k = torch.reshape(k, shape=(batch_size, seq_len, self.num_heads, self.head_dim)).permute([0, 2, 1, 3])\n",
    "        v = torch.reshape(v, shape=(batch_size, seq_len, self.num_heads, self.head_dim)).permute([0, 2, 1, 3])\n",
    "\n",
    "        # score representing how much attention is paid to each word\n",
    "        logits = q @ k.transpose(2, 3) * self.scaling_factor\n",
    "        if mask is not None:\n",
    "            logits = logits.masked_fill(mask, masked_val)\n",
    "        score = F.softmax(logits, dim=-1)\n",
    "        # compute weighted output from values\n",
    "        weighted_v = score @ v\n",
    "        weighted_v = torch.reshape(weighted_v, shape=(batch_size, seq_len, self.embedding_dim))\n",
    "        output = self.multi_head_mapping(weighted_v)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size, img_height, embed_dim = 8, 128, 168\n",
    "attention_module = SelfAttention(embed_dim, num_heads=4)\n",
    "x = torch.randn(size=(batch_size, img_height, embed_dim))\n",
    "y = attention_module(x, x, x)\n",
    "print(\"Output shape: \", y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer Encoder Block"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Transformer Encoder Block consisting of a Multi-Head Self-Attention module, followed by a MLP-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, num_heads: int, hidden_dim: int, dropout_rate: float=0.1, activation: nn.Module=nn.ReLU) -> None:\n",
    "        \"\"\"\n",
    "        :param input_dim: input dimension, generally the word embedding dimension\n",
    "        :param num_heads: number of heads for the multi-head self-attention module\n",
    "        :param hidden_dim: hidden dimension of the 2-layer MLP (input-dim -> hidden_dim -> input_dim)\n",
    "        :param dropout_rate: dropout rate used throughout MLP and Encoder\n",
    "        :param activation: activation function in MLP, defaults to ReLU\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert activation in [nn.ReLU, nn.LeakyReLU, nn.GELU, nn.SELU, nn.ELU, nn.CELU]\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            activation(inplace=True),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "        self.attention_module = SelfAttention(input_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        # residual\n",
    "        x = x + self.dropout(self.attention_module(x, x, x, mask))\n",
    "        x = self.layer_norm1(x)\n",
    "        # second residual\n",
    "        x = x + self.dropout(self.mlp(x))\n",
    "        x = self.layer_norm2(x)\n",
    "        return  x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size, img_height, embed_dim = 8, 128, 168\n",
    "encoder_block = TransformerEncoderBlock(embed_dim, num_heads=4, hidden_dim=4 * embed_dim)\n",
    "\n",
    "x = torch.randn(size=[batch_size, img_height, embed_dim])\n",
    "y = encoder_block(x)\n",
    "print(\"Output shape: \", y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: list, num_layers: int, embedding_dim: int, num_heads: int, mlp_expansion_factor: int=4, dropout_rate: float=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.row_embedding = RowEmbedding(input_dim, embedding_dim)\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding()\n",
    "        self.encoder_blocks = nn.ModuleList([TransformerEncoderBlock(embedding_dim, num_heads, embedding_dim * mlp_expansion_factor, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.row_embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dim = [8, 1, 128, 256]\n",
    "embedding_dim = 168\n",
    "num_layers = 6\n",
    "num_heads = 4\n",
    "\n",
    "encoder = TransformerEncoder(dim, num_layers, embedding_dim, num_heads)\n",
    "\n",
    "t = torch.randn(dim)\n",
    "out = encoder(t)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(f\"Encoder # of parameters: {count_parameters(encoder):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task Head"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Idea - classify each row to a specific class\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        return F.softmax(self.cls(t), dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = 21\n",
    "embed_dim = 168\n",
    "cls_head = ClassificationHead(num_classes, embed_dim)\n",
    "\n",
    "batch_size, img_height = 8, 128\n",
    "t = torch.randn([batch_size, img_height, embed_dim])\n",
    "out = cls_head(t)\n",
    "print(\"Output shape\", out.shape)\n",
    "print(f\"Head # of parameters: {count_parameters(cls_head):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Complete Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClsTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes: int, embed_dim: int, input_dim: list, num_heads: int, num_layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(input_dim, num_layers, embed_dim, num_heads)\n",
    "        self.cls_head =  ClassificationHead(num_classes, embed_dim)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        return self.cls_head(self.encoder(t))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ClsTransformer(num_classes=21, embed_dim=168, input_dim=[8, 1, 128, 256], num_heads=4, num_layers=6)\n",
    "\n",
    "t = torch.randn([8, 1, 128, 256])\n",
    "out = model(t)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(f\"# of parameters: {count_parameters(model):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MNIST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "set_train = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "set_test = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(set_train, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(set_test, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = next(iter(train_loader))\n",
    "print(out[0].shape, out[1].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, train_loader, optim, epoch, device, criterion):\n",
    "    model.train()\n",
    "    for batch_idx, (input, label) in enumerate(train_loader):\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optim.zero_grad()\n",
    "        # todo hack for wrong labels\n",
    "        pred = model(input)[:, 0, :]\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "def test(model, test_loader, device, criterion, epoch):\n",
    "    model.eval()\n",
    "    test_loss, acc = 0, 0\n",
    "    for batch_idx, (input, label) in enumerate(test_loader):\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        pred = model(input)[:, 0, :]\n",
    "        test_loss += criterion(pred, label).sum().item()\n",
    "        cls = pred.argmax(dim=1, keepdim=True)\n",
    "        acc += cls.eq(label.view_as(cls)).sum().item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1} Test-loss: {test_loss / len(test_loader.dataset)} Acc: {acc / len(test_loader.dataset)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = ClsTransformer(10, embed_dim=16, input_dim=[16, 1, 28, 28], num_heads=4, num_layers=6)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(model, train_loader, optim, epoch, device, criterion)\n",
    "    test(model, test_loader, device, criterion, epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "model_path = f\"../model/model_{now.strftime('%d-%m-%y-%H-%M-%S')}.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Run number | Epochs | Description                                                                        | Final Test Loss | Final Test Acc |\n",
    "|------------|--------|------------------------------------------------------------------------------------|-----------------|----------------|\n",
    "| 1          |   10   | ClsTransformer Model ohne positional Encoding mit einfachen RowEmbedding           | 0.937           | 0.9626         |\n",
    "| 2          | 10     | ClsTransformer Model mit sinusoidal positional Encoding mit einfachen RowEmbedding | 0.09423         | 0.9548         |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MNIST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "58.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "80.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "set_train = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "set_test = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(set_train, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(set_test, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "out = next(iter(train_loader))\n",
    "print(out[0].shape, out[1].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def train(model, train_loader, optim, epoch, device, criterion):\n",
    "    model.train()\n",
    "    for batch_idx, (input, label) in enumerate(train_loader):\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optim.zero_grad()\n",
    "        # todo hack for wrong labels\n",
    "        pred = model(input)[:, 0, :]\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "def test(model, test_loader, device, criterion, epoch):\n",
    "    model.eval()\n",
    "    test_loss, acc = 0, 0\n",
    "    for batch_idx, (input, label) in enumerate(test_loader):\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        pred = model(input)[:, 0, :]\n",
    "        test_loss += criterion(pred, label).sum().item()\n",
    "        cls = pred.argmax(dim=1, keepdim=True)\n",
    "        acc += cls.eq(label.view_as(cls)).sum().item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1} Test-loss: {test_loss / len(test_loader.dataset)} Acc: {acc / len(test_loader.dataset)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [28], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m---> 11\u001B[0m     train(model, train_loader, optim, epoch, device, criterion)\n\u001B[1;32m     12\u001B[0m     test(model, test_loader, device, criterion, epoch)\n",
      "Cell \u001B[0;32mIn [25], line 10\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, optim, epoch, device, criterion)\u001B[0m\n\u001B[1;32m      8\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(pred, label)\n\u001B[1;32m      9\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 10\u001B[0m \u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Github/VisionTransformer/venv/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Github/VisionTransformer/venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Github/VisionTransformer/venv/lib/python3.10/site-packages/torch/optim/adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    138\u001B[0m             \u001B[38;5;66;03m# record the step after step update\u001B[39;00m\n\u001B[1;32m    139\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m--> 141\u001B[0m     \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m           \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m           \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m           \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m           \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m           \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/Documents/Github/VisionTransformer/venv/lib/python3.10/site-packages/torch/optim/_functional.py:97\u001B[0m, in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m     94\u001B[0m     grad \u001B[38;5;241m=\u001B[39m grad\u001B[38;5;241m.\u001B[39madd(param, alpha\u001B[38;5;241m=\u001B[39mweight_decay)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m---> 97\u001B[0m \u001B[43mexp_avg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad\u001B[38;5;241m.\u001B[39mconj(), value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amsgrad:\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = ClsTransformer(10, embed_dim=16, input_dim=[16, 1, 28, 28], num_heads=4, num_layers=6)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(model, train_loader, optim, epoch, device, criterion)\n",
    "    test(model, test_loader, device, criterion, epoch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "model_path = f\"../model/model_{now.strftime('%d-%m-%y-%H-%M-%S')}.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Run number | Epochs | Description                                                                        | Final Test Loss | Final Test Acc |\n",
    "|------------|--------|------------------------------------------------------------------------------------|-----------------|----------------|\n",
    "| 1          |   10   | ClsTransformer Model ohne positional Encoding mit einfachen RowEmbedding           | 0.937           | 0.9626         |\n",
    "| 2          | 10     | ClsTransformer Model mit sinusoidal positional Encoding mit einfachen RowEmbedding | 0.09423         | 0.9548         |"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
