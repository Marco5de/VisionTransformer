{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Vision Transformer for Object Detection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'en_US.UTF8'"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, 'en_US.UTF8')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Patch Embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Receives input image (N, C, W, H) (assuming W=H) and turns it into a set of PxP patches\n",
    "    Each patch is mapped linearly to obtain a vector of the flattened patch\n",
    "\n",
    "    Implements equation (1) in the paper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_patches, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = int(input_dim[2] / (num_patches ** 0.5))\n",
    "        self.embed_dim = embedding_dim\n",
    "        # by choosing kernel size and stride to be same as the patch size we divide the image implicitly into the\n",
    "        # desired patches\n",
    "        self.linear_projection = nn.Conv2d(input_dim[1], embedding_dim, kernel_size=self.patch_size,\n",
    "                                           stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # apply projection convolution and concatenate patches -> total of num_patches\n",
    "        x = self.linear_projection(x)\n",
    "        x = torch.reshape(x, [self.input_dim[0], self.num_patches, self.embed_dim])\n",
    "        return x\n",
    "\n",
    "class RowEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # linear is always applied to last dimension (BxCxWxH)\n",
    "        # for now single projection is shared for each row!\n",
    "        self.proj = nn.Linear(input_dim[-1], embedding_dim, bias=True)\n",
    "        # Todo - missing positional encdoing\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        return torch.squeeze(self.proj(t))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 9,437,952\n",
      "Output shape: torch.Size([16, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "dim = [16, 3, 256, 256]\n",
    "num_patches = 16\n",
    "embedding_dim = 768\n",
    "t = torch.randn(dim)\n",
    "patch_embedding = PatchEmbedding(dim, num_patches, embedding_dim)\n",
    "print(f\"Number of parameters: {count_parameters(patch_embedding):,}\")\n",
    "out = patch_embedding(t)\n",
    "print(\"Output shape:\", out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([16, 1, 128, 168])\n",
      "Number of parameters: 43,176\n"
     ]
    }
   ],
   "source": [
    "dim = [16, 1, 128, 256]\n",
    "embedding_dim = 168\n",
    "t = torch.randn(dim)\n",
    "row_embedding = RowEmbedding(dim, embedding_dim)\n",
    "out = row_embedding(t)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(f\"Number of parameters: {count_parameters(row_embedding):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Masked Self-Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Head Self-Attention module as described in the Attention is all you need paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, num_heads: int) -> None:\n",
    "        \"\"\"\n",
    "        :param embedding_dim: dimension of the word embeddings, must be dividable by num_heads\n",
    "        :param num_heads: number of attention heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert embedding_dim % num_heads == 0, \"Embedding dim must be dividable by number of attention heads!\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.embedding_dim // self.num_heads\n",
    "\n",
    "        # mapping matrices W_Q, W_K, W_V for queries, keys, values\n",
    "        # doing this in a single linear mapping for qkv is more efficient, but this notebook is mainly for readability\n",
    "        self.query_mapping = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.key_mapping = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        self.value_mapping = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "        # scaling factor for more stable gradients when using the softmax function -> refer to paper for details\n",
    "        self.scaling_factor = math.sqrt(embedding_dim)\n",
    "\n",
    "        self.multi_head_mapping = nn.Linear(embedding_dim, embedding_dim, bias=True)\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2:torch.Tensor, x3: torch.Tensor, mask: torch.Tensor= None, masked_val: float=1e15) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x1.shape\n",
    "        q = self.query_mapping(x1)\n",
    "        k = self.key_mapping(x2)\n",
    "        v = self.value_mapping(x3)\n",
    "        # reshape to heads format and [batch_size, seq_len, num_heads, head_dim] -> [batch_size, seq_len, num_heads, head_dim]\n",
    "        q = torch.reshape(q, shape=(batch_size, seq_len, self.num_heads, self.head_dim)).permute([0, 2, 1, 3])\n",
    "        k = torch.reshape(k, shape=(batch_size, seq_len, self.num_heads, self.head_dim)).permute([0, 2, 1, 3])\n",
    "        v = torch.reshape(v, shape=(batch_size, seq_len, self.num_heads, self.head_dim)).permute([0, 2, 1, 3])\n",
    "\n",
    "        # score representing how much attention is paid to each word\n",
    "        logits = q @ k.transpose(2, 3) * self.scaling_factor\n",
    "        if mask is not None:\n",
    "            logits = logits.masked_fill(mask, masked_val)\n",
    "        score = F.softmax(logits, dim=-1)\n",
    "        # compute weighted output from values\n",
    "        weighted_v = score @ v\n",
    "        weighted_v = torch.reshape(weighted_v, shape=(batch_size, seq_len, self.embedding_dim))\n",
    "        output = self.multi_head_mapping(weighted_v)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([8, 128, 168])\n"
     ]
    }
   ],
   "source": [
    "batch_size, img_height, embed_dim = 8, 128, 168\n",
    "attention_module = SelfAttention(embed_dim, num_heads=4)\n",
    "x = torch.randn(size=(batch_size, img_height, embed_dim))\n",
    "y = attention_module(x, x, x)\n",
    "print(\"Output shape: \", y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer Encoder Block"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Transformer Encoder Block consisting of a Multi-Head Self-Attention module, followed by a MLP-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, num_heads: int, hidden_dim: int, dropout_rate: float=0.1, activation: nn.Module=nn.ReLU) -> None:\n",
    "        \"\"\"\n",
    "        :param input_dim: input dimension, generally the word embedding dimension\n",
    "        :param num_heads: number of heads for the multi-head self-attention module\n",
    "        :param hidden_dim: hidden dimension of the 2-layer MLP (input-dim -> hidden_dim -> input_dim)\n",
    "        :param dropout_rate: dropout rate used throughout MLP and Encoder\n",
    "        :param activation: activation function in MLP, defaults to ReLU\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert activation in [nn.ReLU, nn.LeakyReLU, nn.GELU, nn.SELU, nn.ELU, nn.CELU]\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            activation(inplace=True),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "        self.attention_module = SelfAttention(input_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        # residual\n",
    "        x = x + self.dropout(self.attention_module(x, x, x, mask))\n",
    "        x = self.layer_norm1(x)\n",
    "        # second residual\n",
    "        x = x + self.dropout(self.mlp(x))\n",
    "        x = self.layer_norm2(x)\n",
    "        return  x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([8, 128, 168])\n"
     ]
    }
   ],
   "source": [
    "batch_size, img_height, embed_dim = 8, 128, 168\n",
    "encoder_block = TransformerEncoderBlock(embed_dim, num_heads=4, hidden_dim=4 * embed_dim)\n",
    "\n",
    "x = torch.randn(size=[batch_size, img_height, embed_dim])\n",
    "y = encoder_block(x)\n",
    "print(\"Output shape: \", y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers: int, embedding_dim: int, num_heads: int, mlp_expansion_factor: int=4, dropout_rate: float=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.row_embedding = RowEmbedding(input_dim, embedding_dim)\n",
    "        self.encoder_blocks = nn.ModuleList([TransformerEncoderBlock(embedding_dim, num_heads, embedding_dim * mlp_expansion_factor, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.row_embedding(x)\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 128, 168])\n",
      "Encoder # of parameters: 2,088,408\n"
     ]
    }
   ],
   "source": [
    "dim = [8, 1, 128, 256]\n",
    "embedding_dim = 168\n",
    "num_layers = 6\n",
    "num_heads = 4\n",
    "\n",
    "encoder = TransformerEncoder(dim, num_layers, embedding_dim, num_heads)\n",
    "\n",
    "t = torch.randn(dim)\n",
    "out = encoder(t)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(f\"Encoder # of parameters: {count_parameters(encoder):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task Head"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Idea - classify each row to a specific class\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        return F.softmax(self.cls(t), dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape torch.Size([8, 128, 21])\n",
      "Head # of parameters: 3,549\n"
     ]
    }
   ],
   "source": [
    "num_classes = 21\n",
    "embed_dim = 168\n",
    "cls_head = ClassificationHead(num_classes, embed_dim)\n",
    "\n",
    "batch_size, img_height = 8, 128\n",
    "t = torch.randn([batch_size, img_height, embed_dim])\n",
    "out = cls_head(t)\n",
    "print(\"Output shape\", out.shape)\n",
    "print(f\"Head # of parameters: {count_parameters(cls_head):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Complete Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "class ClsTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, embed_dim, input_dim, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(dim, num_layers, embed_dim, num_heads)\n",
    "        self.cls_head =  ClassificationHead(num_classes, embed_dim)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        return self.cls_head(self.encoder(t))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 128, 21])\n",
      "# of parameters: 2,091,957\n"
     ]
    }
   ],
   "source": [
    "model = ClsTransformer(num_classes=21, embed_dim=168, input_dim=[8, 1, 128, 256], num_heads=4, num_layers=6)\n",
    "\n",
    "t = torch.randn([8, 1, 128, 256])\n",
    "out = model(t)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(f\"# of parameters: {count_parameters(model):,}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}